{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from suzaki.seq2seq import Encoder, Seq2seq\n",
    "from suzaki.attention_layer import TimeAttention\n",
    "\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Seq2Se2の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout\n",
    "\n",
    "\n",
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled\n",
    "\n",
    "\n",
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention_Layerの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1)#.repeat(T, axis=1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "\n",
    "        return dhs, da\n",
    "\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh\n",
    "\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh\n",
    "\n",
    "\n",
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from suzaki.seq2seq import Seq2seq\n",
    "from suzaki.peeky_seq2seq import PeekySeq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習させて、グラフを描画する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
      "| epoch 1 |  iter 21 / 351 | time 6[s] | loss 3.09\n",
      "| epoch 1 |  iter 41 / 351 | time 11[s] | loss 1.90\n",
      "| epoch 1 |  iter 61 / 351 | time 17[s] | loss 1.72\n",
      "| epoch 1 |  iter 81 / 351 | time 24[s] | loss 1.46\n",
      "| epoch 1 |  iter 101 / 351 | time 30[s] | loss 1.19\n",
      "| epoch 1 |  iter 121 / 351 | time 37[s] | loss 1.14\n",
      "| epoch 1 |  iter 141 / 351 | time 44[s] | loss 1.09\n",
      "| epoch 1 |  iter 161 / 351 | time 50[s] | loss 1.06\n",
      "| epoch 1 |  iter 181 / 351 | time 56[s] | loss 1.04\n",
      "| epoch 1 |  iter 201 / 351 | time 63[s] | loss 1.03\n",
      "| epoch 1 |  iter 221 / 351 | time 69[s] | loss 1.02\n",
      "| epoch 1 |  iter 241 / 351 | time 75[s] | loss 1.02\n",
      "| epoch 1 |  iter 261 / 351 | time 81[s] | loss 1.01\n",
      "| epoch 1 |  iter 281 / 351 | time 87[s] | loss 1.00\n",
      "| epoch 1 |  iter 301 / 351 | time 92[s] | loss 1.00\n",
      "| epoch 1 |  iter 321 / 351 | time 98[s] | loss 1.00\n",
      "| epoch 1 |  iter 341 / 351 | time 104[s] | loss 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 1978-08-11\n",
      "---\n",
      "val acc 0.000%\n",
      "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.00\n",
      "| epoch 2 |  iter 21 / 351 | time 6[s] | loss 1.00\n",
      "| epoch 2 |  iter 41 / 351 | time 14[s] | loss 0.99\n",
      "| epoch 2 |  iter 61 / 351 | time 21[s] | loss 0.99\n",
      "| epoch 2 |  iter 81 / 351 | time 27[s] | loss 0.99\n",
      "| epoch 2 |  iter 101 / 351 | time 40[s] | loss 0.99\n",
      "| epoch 2 |  iter 121 / 351 | time 46[s] | loss 0.99\n",
      "| epoch 2 |  iter 141 / 351 | time 53[s] | loss 0.98\n",
      "| epoch 2 |  iter 161 / 351 | time 60[s] | loss 0.98\n",
      "| epoch 2 |  iter 181 / 351 | time 67[s] | loss 0.97\n",
      "| epoch 2 |  iter 201 / 351 | time 73[s] | loss 0.95\n",
      "| epoch 2 |  iter 221 / 351 | time 79[s] | loss 0.94\n",
      "| epoch 2 |  iter 241 / 351 | time 87[s] | loss 0.90\n",
      "| epoch 2 |  iter 261 / 351 | time 93[s] | loss 0.83\n",
      "| epoch 2 |  iter 281 / 351 | time 101[s] | loss 0.74\n",
      "| epoch 2 |  iter 301 / 351 | time 107[s] | loss 0.66\n",
      "| epoch 2 |  iter 321 / 351 | time 114[s] | loss 0.58\n",
      "| epoch 2 |  iter 341 / 351 | time 120[s] | loss 0.46\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 2016-11-08\n",
      "---\n",
      "val acc 51.760%\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.35\n",
      "| epoch 3 |  iter 21 / 351 | time 5[s] | loss 0.30\n",
      "| epoch 3 |  iter 41 / 351 | time 11[s] | loss 0.21\n",
      "| epoch 3 |  iter 61 / 351 | time 17[s] | loss 0.14\n",
      "| epoch 3 |  iter 81 / 351 | time 23[s] | loss 0.09\n",
      "| epoch 3 |  iter 101 / 351 | time 29[s] | loss 0.07\n",
      "| epoch 3 |  iter 121 / 351 | time 36[s] | loss 0.05\n",
      "| epoch 3 |  iter 141 / 351 | time 41[s] | loss 0.04\n",
      "| epoch 3 |  iter 161 / 351 | time 47[s] | loss 0.03\n",
      "| epoch 3 |  iter 181 / 351 | time 53[s] | loss 0.03\n",
      "| epoch 3 |  iter 201 / 351 | time 59[s] | loss 0.02\n",
      "| epoch 3 |  iter 221 / 351 | time 65[s] | loss 0.02\n",
      "| epoch 3 |  iter 241 / 351 | time 74[s] | loss 0.02\n",
      "| epoch 3 |  iter 261 / 351 | time 81[s] | loss 0.01\n",
      "| epoch 3 |  iter 281 / 351 | time 88[s] | loss 0.01\n",
      "| epoch 3 |  iter 301 / 351 | time 94[s] | loss 0.01\n",
      "| epoch 3 |  iter 321 / 351 | time 100[s] | loss 0.01\n",
      "| epoch 3 |  iter 341 / 351 | time 105[s] | loss 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 99.900%\n",
      "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 0.01\n",
      "| epoch 4 |  iter 21 / 351 | time 5[s] | loss 0.01\n",
      "| epoch 4 |  iter 41 / 351 | time 11[s] | loss 0.01\n",
      "| epoch 4 |  iter 61 / 351 | time 16[s] | loss 0.01\n",
      "| epoch 4 |  iter 81 / 351 | time 22[s] | loss 0.01\n",
      "| epoch 4 |  iter 101 / 351 | time 27[s] | loss 0.01\n",
      "| epoch 4 |  iter 121 / 351 | time 33[s] | loss 0.00\n",
      "| epoch 4 |  iter 141 / 351 | time 39[s] | loss 0.01\n",
      "| epoch 4 |  iter 161 / 351 | time 45[s] | loss 0.00\n",
      "| epoch 4 |  iter 181 / 351 | time 52[s] | loss 0.00\n",
      "| epoch 4 |  iter 201 / 351 | time 58[s] | loss 0.00\n",
      "| epoch 4 |  iter 221 / 351 | time 64[s] | loss 0.00\n",
      "| epoch 4 |  iter 241 / 351 | time 70[s] | loss 0.00\n",
      "| epoch 4 |  iter 261 / 351 | time 76[s] | loss 0.00\n",
      "| epoch 4 |  iter 281 / 351 | time 82[s] | loss 0.00\n",
      "| epoch 4 |  iter 301 / 351 | time 89[s] | loss 0.00\n",
      "| epoch 4 |  iter 321 / 351 | time 96[s] | loss 0.00\n",
      "| epoch 4 |  iter 341 / 351 | time 101[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 99.940%\n",
      "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 5 |  iter 21 / 351 | time 6[s] | loss 0.00\n",
      "| epoch 5 |  iter 41 / 351 | time 12[s] | loss 0.00\n",
      "| epoch 5 |  iter 61 / 351 | time 17[s] | loss 0.00\n",
      "| epoch 5 |  iter 81 / 351 | time 23[s] | loss 0.00\n",
      "| epoch 5 |  iter 101 / 351 | time 29[s] | loss 0.00\n",
      "| epoch 5 |  iter 121 / 351 | time 34[s] | loss 0.00\n",
      "| epoch 5 |  iter 141 / 351 | time 40[s] | loss 0.00\n",
      "| epoch 5 |  iter 161 / 351 | time 45[s] | loss 0.00\n",
      "| epoch 5 |  iter 181 / 351 | time 51[s] | loss 0.00\n",
      "| epoch 5 |  iter 201 / 351 | time 57[s] | loss 0.00\n",
      "| epoch 5 |  iter 221 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 5 |  iter 241 / 351 | time 68[s] | loss 0.00\n",
      "| epoch 5 |  iter 261 / 351 | time 73[s] | loss 0.02\n",
      "| epoch 5 |  iter 281 / 351 | time 79[s] | loss 0.01\n",
      "| epoch 5 |  iter 301 / 351 | time 84[s] | loss 0.00\n",
      "| epoch 5 |  iter 321 / 351 | time 90[s] | loss 0.00\n",
      "| epoch 5 |  iter 341 / 351 | time 97[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 99.960%\n",
      "| epoch 6 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 6 |  iter 21 / 351 | time 7[s] | loss 0.00\n",
      "| epoch 6 |  iter 41 / 351 | time 17[s] | loss 0.00\n",
      "| epoch 6 |  iter 61 / 351 | time 24[s] | loss 0.00\n",
      "| epoch 6 |  iter 81 / 351 | time 30[s] | loss 0.00\n",
      "| epoch 6 |  iter 101 / 351 | time 36[s] | loss 0.00\n",
      "| epoch 6 |  iter 121 / 351 | time 41[s] | loss 0.00\n",
      "| epoch 6 |  iter 141 / 351 | time 48[s] | loss 0.00\n",
      "| epoch 6 |  iter 161 / 351 | time 54[s] | loss 0.00\n",
      "| epoch 6 |  iter 181 / 351 | time 61[s] | loss 0.00\n",
      "| epoch 6 |  iter 201 / 351 | time 67[s] | loss 0.00\n",
      "| epoch 6 |  iter 221 / 351 | time 73[s] | loss 0.00\n",
      "| epoch 6 |  iter 241 / 351 | time 80[s] | loss 0.00\n",
      "| epoch 6 |  iter 261 / 351 | time 87[s] | loss 0.00\n",
      "| epoch 6 |  iter 281 / 351 | time 94[s] | loss 0.00\n",
      "| epoch 6 |  iter 301 / 351 | time 100[s] | loss 0.00\n",
      "| epoch 6 |  iter 321 / 351 | time 107[s] | loss 0.00\n",
      "| epoch 6 |  iter 341 / 351 | time 113[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| epoch 7 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 7 |  iter 21 / 351 | time 6[s] | loss 0.00\n",
      "| epoch 7 |  iter 41 / 351 | time 12[s] | loss 0.00\n",
      "| epoch 7 |  iter 61 / 351 | time 18[s] | loss 0.00\n",
      "| epoch 7 |  iter 81 / 351 | time 23[s] | loss 0.00\n",
      "| epoch 7 |  iter 101 / 351 | time 29[s] | loss 0.00\n",
      "| epoch 7 |  iter 121 / 351 | time 34[s] | loss 0.00\n",
      "| epoch 7 |  iter 141 / 351 | time 40[s] | loss 0.00\n",
      "| epoch 7 |  iter 161 / 351 | time 45[s] | loss 0.00\n",
      "| epoch 7 |  iter 181 / 351 | time 51[s] | loss 0.00\n",
      "| epoch 7 |  iter 201 / 351 | time 57[s] | loss 0.00\n",
      "| epoch 7 |  iter 221 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 7 |  iter 241 / 351 | time 68[s] | loss 0.00\n",
      "| epoch 7 |  iter 261 / 351 | time 74[s] | loss 0.00\n",
      "| epoch 7 |  iter 281 / 351 | time 82[s] | loss 0.00\n",
      "| epoch 7 |  iter 301 / 351 | time 87[s] | loss 0.00\n",
      "| epoch 7 |  iter 321 / 351 | time 93[s] | loss 0.00\n",
      "| epoch 7 |  iter 341 / 351 | time 99[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| epoch 8 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 8 |  iter 21 / 351 | time 5[s] | loss 0.00\n",
      "| epoch 8 |  iter 41 / 351 | time 11[s] | loss 0.00\n",
      "| epoch 8 |  iter 61 / 351 | time 17[s] | loss 0.00\n",
      "| epoch 8 |  iter 81 / 351 | time 22[s] | loss 0.00\n",
      "| epoch 8 |  iter 101 / 351 | time 28[s] | loss 0.00\n",
      "| epoch 8 |  iter 121 / 351 | time 33[s] | loss 0.00\n",
      "| epoch 8 |  iter 141 / 351 | time 39[s] | loss 0.00\n",
      "| epoch 8 |  iter 161 / 351 | time 45[s] | loss 0.00\n",
      "| epoch 8 |  iter 181 / 351 | time 50[s] | loss 0.00\n",
      "| epoch 8 |  iter 201 / 351 | time 56[s] | loss 0.00\n",
      "| epoch 8 |  iter 221 / 351 | time 61[s] | loss 0.00\n",
      "| epoch 8 |  iter 241 / 351 | time 67[s] | loss 0.00\n",
      "| epoch 8 |  iter 261 / 351 | time 73[s] | loss 0.00\n",
      "| epoch 8 |  iter 281 / 351 | time 79[s] | loss 0.00\n",
      "| epoch 8 |  iter 301 / 351 | time 84[s] | loss 0.00\n",
      "| epoch 8 |  iter 321 / 351 | time 90[s] | loss 0.00\n",
      "| epoch 8 |  iter 341 / 351 | time 96[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| epoch 9 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 9 |  iter 21 / 351 | time 6[s] | loss 0.00\n",
      "| epoch 9 |  iter 41 / 351 | time 12[s] | loss 0.00\n",
      "| epoch 9 |  iter 61 / 351 | time 17[s] | loss 0.00\n",
      "| epoch 9 |  iter 81 / 351 | time 23[s] | loss 0.00\n",
      "| epoch 9 |  iter 101 / 351 | time 29[s] | loss 0.00\n",
      "| epoch 9 |  iter 121 / 351 | time 34[s] | loss 0.00\n",
      "| epoch 9 |  iter 141 / 351 | time 40[s] | loss 0.00\n",
      "| epoch 9 |  iter 161 / 351 | time 46[s] | loss 0.00\n",
      "| epoch 9 |  iter 181 / 351 | time 51[s] | loss 0.00\n",
      "| epoch 9 |  iter 201 / 351 | time 57[s] | loss 0.00\n",
      "| epoch 9 |  iter 221 / 351 | time 62[s] | loss 0.00\n",
      "| epoch 9 |  iter 241 / 351 | time 68[s] | loss 0.00\n",
      "| epoch 9 |  iter 261 / 351 | time 74[s] | loss 0.00\n",
      "| epoch 9 |  iter 281 / 351 | time 79[s] | loss 0.00\n",
      "| epoch 9 |  iter 301 / 351 | time 85[s] | loss 0.00\n",
      "| epoch 9 |  iter 321 / 351 | time 90[s] | loss 0.00\n",
      "| epoch 9 |  iter 341 / 351 | time 96[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| epoch 10 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 10 |  iter 21 / 351 | time 6[s] | loss 0.00\n",
      "| epoch 10 |  iter 41 / 351 | time 13[s] | loss 0.00\n",
      "| epoch 10 |  iter 61 / 351 | time 20[s] | loss 0.00\n",
      "| epoch 10 |  iter 81 / 351 | time 26[s] | loss 0.00\n",
      "| epoch 10 |  iter 101 / 351 | time 33[s] | loss 0.00\n",
      "| epoch 10 |  iter 121 / 351 | time 39[s] | loss 0.00\n",
      "| epoch 10 |  iter 141 / 351 | time 46[s] | loss 0.00\n",
      "| epoch 10 |  iter 161 / 351 | time 52[s] | loss 0.00\n",
      "| epoch 10 |  iter 181 / 351 | time 59[s] | loss 0.00\n",
      "| epoch 10 |  iter 201 / 351 | time 66[s] | loss 0.00\n",
      "| epoch 10 |  iter 221 / 351 | time 72[s] | loss 0.00\n",
      "| epoch 10 |  iter 241 / 351 | time 79[s] | loss 0.00\n",
      "| epoch 10 |  iter 261 / 351 | time 86[s] | loss 0.00\n",
      "| epoch 10 |  iter 281 / 351 | time 93[s] | loss 0.00\n",
      "| epoch 10 |  iter 301 / 351 | time 100[s] | loss 0.00\n",
      "| epoch 10 |  iter 321 / 351 | time 107[s] | loss 0.00\n",
      "| epoch 10 |  iter 341 / 351 | time 113[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 100.000%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11197ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 入力文を反転\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()\n",
    "\n",
    "# グラフの描画\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
