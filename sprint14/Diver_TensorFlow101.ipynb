{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sprint14-dnn-framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegressionの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習用のトレーニングデータを生成します。\n",
    "ANDゲートでは入力が2次元で出力が1次元となります。 \n",
    "2つの入力が1のときだけ1を出力し、それ以外は0を出力するので以下のように定義できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_train = np.array([[0],[0],[0],[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力データxと正解出力データtを変数tf.placeholder()を使って定義していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "t = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_10:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_11:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一引数のtf.float32で行列要素の数値のデータ型を指定しています。第二引数の[None,2]で行列の形を指定しています。 \n",
    "ここで定義されている2はデータの次元を表しています。Noneの部分はデータ数を表す部分です。\n",
    "\n",
    "今回のANDゲートの場合のデータ数は[0,0],[0,1],[1,0],[1,1]の4つしかないのでNoneの部分を[4,2]としても問題はありません。\n",
    "\n",
    "しかし、ここで定義したxとtはまだ値が何も入っておらず、ただの入れ物にすぎません。\n",
    "\n",
    "パラメータの最適化処理の際には、データを一部だけtf.placeholder()に入れて計算することもあります。そのような、データ数が可変の場合に任意の数のデータを入れられるように、一般的にはNoneを使います。\n",
    "\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重みとバイアスは変数tf.Valiable()を使って定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([2, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_10:0' shape=(2, 1) dtype=float32_ref>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_11:0' shape=(1,) dtype=float32_ref>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、tf.Variable()の中でtf.zeros()という関数を呼び出していますが、変数の初期値を0にしています。つまり2×1行列のすべての要素が0に設定されます。この初期値の状態からWとbの値を学習していくことが今回の目的となります。\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/Variable\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次にモデルの出力yを定義します。 \n",
    "LogisticRegressionの出力yは活性化関数であるシグモイド関数を使って出力させます。DeepLearning入門1でLogisticRegressionとPerceptronは若干異なると説明しましたが、その違いはここになります。Perceptronの場合には活性化関数にステップ関数(入力が0より大きければ1、それ以外であれば0を出力する関数)を使用します。\n",
    "では実装してみましょう。普段、関数を使う場合は"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "のように定義してから使います。しかし、TensorFlowの場合は、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.nn.sigmoid(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と書けば定義しなくても実装することができます。sigmoidの中のtf.matmul(x,W)はxとWの行列の積を表しています。 \n",
    "別の例として、ソフトマックス関数を使いたい場合は、y = tf.nn.softmax(tf.matmul(x,W) + b)とします。\n",
    "\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの出力を定義した後は、誤差関数を定義します。\n",
    "今回は交差エントロピー誤差関数を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(t * tf.log(y) + (1 - t) * tf.log(1 - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.reduce_sum()で行列の要素の総和を計算しています。今回は交差エントロピー誤差関数を使いましたが、もしも二乗和誤差関数を使用したければtf.reduce_sum(tf.square(y - t))と書けば定義できます。tf.square()は行列の要素ごとの二乗を計算します。この2つからわかるようにTensorFlowでは、数式の見た目とほぼ同じように誤差関数を定義することができました。\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/square\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/log\n",
    "\n",
    "    \n",
    "## GradientDescent(勾配降下法)を用いてパラメータを最適化\n",
    "\n",
    "誤差関数は最小化させることに意味がありました。次はGradientDescent(勾配降下法)を用いてパラメータを最適化しましょう。GradientDescentについては次章DeepLearning入門5で詳しく扱いますのでここでは誤差関数を最小化している(正解データと出力データの誤差を最小化している)と言うことだけ理解して読み進めてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientDescentOptimizer()は引数で学習率を指定しています。学習率についても次章で扱います。\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer\n",
    "    \n",
    "    \n",
    "ここまでで学習のための式の定義は終わりです。 \n",
    "\n",
    "## 正解率の表示\n",
    "ここで学習後の結果の正解が正しいかどうかの判定と正解率を表示させておきたいと思います。TensorFlowの関数を使って式を定義していきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.sign(y-0.5),tf.sign(t-0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1行目で結果が正解かどうか判定しています。一つ一つ見ていきましょう。 \n",
    "\n",
    "まずtf.equal()は引数に指定された2つの値が等しいかどうかを判定してくれます。返り値はBool値です。\n",
    "\n",
    "tf.sign()は引数の値が正なら1、0なら0、負なら-1を返します。yが0.5以上かどうかで結果が決まるので、y-0.5とt-0.5の符号を比較しています。\n",
    "\n",
    "TensorFlowでは値を浮動少数点で扱っているため、出力yの値がぴったり0になることはほぼありえません。 なのでここでは引数の符号が+か-どうかで判定しています。\n",
    "\n",
    "\n",
    "\n",
    "2行目は正解率を計算するためのコードです。 \n",
    "\n",
    "tf.reduce_mean()引数に指定された多次元配列の各成分の平均を計算する関数です。 \n",
    "\n",
    "tf.cast()でBool値を0,1に変換しています。 \n",
    "\n",
    "つまりここでは正解で1、不正解で0と判定された配列の平均値をとっているので正解率を表していることになります。\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/equal\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/sign\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/reduce_mean\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/cast\n",
    "\n",
    "## セッションを準備してパラメータを最適化\n",
    "ここまでできたら、最後にセッションを準備してパラメータを最適化していきましょう。\n",
    "\n",
    "TensorFlowでは必ずセッションを定義してパラメータ(重み、バイアス)を計算していきます。簡単なのでコードを見ていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずtf.Session()を変数に格納します。そしてsess.run()でセッションの実行、tf.global_variables_initializer()で上で定義したtf.Variable()の値(重み、バイアス)が初期化されます。\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer\n",
    "\n",
    "## 最適化の繰り返し回数(エポック数)を指定\n",
    "セッションを準備したら最適化の繰り返し回数(エポック数)を指定していきます。今回はパラメータ最適化処理を1000回繰り返すことにします。エポック数とその時点での正解率も同時に表示できるようにしておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Accuracy: 0.750000\n",
      "epoch: 100, Accuracy: 1.000000\n",
      "epoch: 200, Accuracy: 1.000000\n",
      "epoch: 300, Accuracy: 1.000000\n",
      "epoch: 400, Accuracy: 1.000000\n",
      "epoch: 500, Accuracy: 1.000000\n",
      "epoch: 600, Accuracy: 1.000000\n",
      "epoch: 700, Accuracy: 1.000000\n",
      "epoch: 800, Accuracy: 1.000000\n",
      "epoch: 900, Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    sess.run(train_step, feed_dict={x:x_train, t:y_train})\n",
    "    #ここから下は100エポックごとに正解率を表示させています。\n",
    "    #１００で割った余りがゼロだったら\n",
    "    if epoch % 100 == 0:\n",
    "        acc_val = sess.run(\n",
    "            accuracy, feed_dict={\n",
    "                x:x_train,\n",
    "                t:y_train})\n",
    "        print ('epoch: %d, Accuracy: %f'\n",
    "                %(epoch, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず1行目、エポック数を1000回に指定しています。\n",
    "\n",
    "2行目、sess.run()の引数にtrain_stepを入れることで上で定義したtrain_stepを実行しています。\n",
    "\n",
    "つまり勾配降下法によって実際に学習させるのがこの部分になります。\n",
    "\n",
    "\n",
    "この時点で形だけ定義していたplaceholder(xとt)の中にはまだ何も入っていません。\n",
    "\n",
    "placeholderに値を設定するためにsess.run()のパラメータにfeed_dictというものを使用します。\n",
    "\n",
    "feed_dict={x:x_train,t:y_train})と書くことで空箱だったxにx_trainの値が入り、tにy_trainの値が入ります。\n",
    "\n",
    "\n",
    "正解率の表示に関してもcorrect_predictionやaccuracyにまだ値が入っていないためfeed_dictを指定します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はモデルが簡単すぎたため、100回目でもうすでにAccuracyが100%になりました。\n",
    "\n",
    "モデルが複雑になると100%が出ることはほぼあり得ません。\n",
    "\n",
    "## 結果確認\n",
    "では、一旦この表示結果を見てみましょう。\n",
    "\n",
    "最後に結果を確認してLogisticRegressionの実装を終わりましょう。結果の確認には.eval()を使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習結果が正しいか確認する\n",
    "classified = correct_prediction.eval(session=sess, feed_dict ={\n",
    "    x:x_train, \n",
    "    t:y_train\n",
    "})\n",
    "#出力yの確認\n",
    "prob = y.eval(session=sess, feed_dict={\n",
    "    x:x_train,\n",
    "    t:y_train\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必ず引数でsessionを指定しましょう。また、今回もcorrect_predictionとyには値が入っていない状態のためfeed_dictで値を入れましょう。\n",
    "## 結果は"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "print(classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.9651421e-04]\n",
      " [4.9049832e-02]\n",
      " [4.9049832e-02]\n",
      " [9.3120378e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "これの見方ですが、eは＊１０という意味で、-04はマイナス４乗の意味になります。\n",
    "\n",
    "- 1.9に１０のマイナス4乗かけるということは、点を左に2回進めることになるので、0.00019になるので、ほぼ0です。\n",
    "- 4.9に１０のマイナス2乗かけるということは、点を左に2回進めることになるので、0.049になるので、ほぼ0です。\n",
    "- 9.3に１０のマイナス1乗かけるということは、点を左に1回進めることになるので、0.93になるので、ほぼ1です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifiedの結果は全てTrueで正しく学習されていることがわかります。\n",
    "probは上からほぼ[0,0,0,1]となっていることがわかります。今回活性化関数に用いたのはシグモイド関数ですので出力yは確率として表示されています。\n",
    "\n",
    "ANDゲートの正解出力tが[0,0,0,1]となるので混乱しやすいですが、\n",
    "\n",
    "yはニューロンが発火する(1)か発火しない(0)かを確率で判断しています。なので意味としては上から3つ→1になる確率がほぼ0％、一番下の1つ→93％程度の確率で1になるということを表しています。\n",
    "\n",
    "\n",
    "## Wとbが学習後どのような値になっているかも見ておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[5.5699544]\n",
      " [5.5699544]]\n",
      "b: [-8.534579]\n"
     ]
    }
   ],
   "source": [
    "print('W:', sess.run(W))\n",
    "print('b:', sess.run(b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wとbの学習結果を表示させるためには、sess.run()を使えば値を得ることができます。なぜ今回はeval()ではなくsess.run()なのかはtf.Valiable()で定義しているかしていないかです。tf.Valiable()で定義していればsess.run()で取得した値が返ってきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XORゲートの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここからはMLPを実装してXORゲートの学習の学習をしていきます。XORゲートは線形分離が不可能です。なので前回学んだLogisticRegressionではXORゲートを学習することができません。ではどうするのか、今回は新たに隠れ層も実装していきます。MLPの実装を通してTensorFlowでの隠れ層の使い方を学んでいきましょう。とは言っても、ここで基本となるのは前回のLogisticRegressionで学んだことです。変数の使い方など基本的な部分は同じです。ではコードを見ていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XORゲートでは2つの入力のどちらかだけ1の場合1を出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_train = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力2次元、出力1次元は前回と同じなので"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None,2])\n",
    "t = tf.placeholder(tf.float32, [None,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次は前回と違う点、隠れ層の設定をしていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.truncated_normal([2,2]))\n",
    "b1 = tf.Variable(tf.zeros([2]))\n",
    "h = tf.nn.sigmoid(tf.matmul(x,W1) + b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W1は一層目の重みを表しています。W1の引数であるtf.truncated_normal()は切断正規分布に基づいたデータを生成させています。\n",
    "\n",
    "切断正規分布は、ここでは誤差逆伝播法をうまく作用させるために乱数で初期化しているということだけ押さえておきましょう。\n",
    "\n",
    "LogisticRegressionのときにようにパラメータをすべて0で初期化してしまうと誤差逆伝播法がうまく作用されないことがあります。\n",
    "\n",
    "（もちろんtruncated_normal()ではなく、random_normal()でも作用します。）\n",
    "\n",
    "その中の[2,2]は[隠れ層の入力次元,隠れ層のノードの数]を指定しています。 \n",
    "\n",
    "b1は入力層から隠れ層に入力されるバイアスです。\n",
    "\n",
    "ここで指定されている[2]も隠れ層のノード数を指定しています。\n",
    "\n",
    "そしてhが隠れ層を表します。層を追加することの利点は、非線形関数を用いることで、線形関数では対処できない問題に対して対処できるように値を変換することでした。ここではシグモイド関数を使って隠れ層に入ってきた重み付き和を変換します。\n",
    "\n",
    "ここでは隠れ層は一層しか導入しませんが、さらに層を追加したい場合も同じように定義していけば実装することができます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.truncated_normal([2,1]))\n",
    "b2 = tf.Variable(tf.zeros([1]))\n",
    "y = tf.nn.sigmoid(tf.matmul(h, W2) + b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コードの意味は先程の隠れ層と同じです。コードを見てみるとノードの数が変化していることがわかります。\n",
    "\n",
    "今定義しているのは出力層なので、これは出力層のノードの数が1つであることを意味します。\n",
    "\n",
    "誤差関数はPerceptronと同様に交差エントロピー誤差関数を使い、勾配降下法の実装も前回と同じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(t * tf.log(y) + (1 - t) * tf.log(1 - y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.sign(y-0.5),tf.sign(t-0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習過程の実装も前回と同様"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Accuracy: 0.500000\n",
      "epoch: 100, Accuracy: 0.500000\n",
      "epoch: 200, Accuracy: 0.500000\n",
      "epoch: 300, Accuracy: 0.500000\n",
      "epoch: 400, Accuracy: 0.500000\n",
      "epoch: 500, Accuracy: 0.500000\n",
      "epoch: 600, Accuracy: 0.500000\n",
      "epoch: 700, Accuracy: 0.750000\n",
      "epoch: 800, Accuracy: 0.750000\n",
      "epoch: 900, Accuracy: 0.750000\n",
      "epoch: 1000, Accuracy: 0.750000\n",
      "epoch: 1100, Accuracy: 0.750000\n",
      "epoch: 1200, Accuracy: 0.750000\n",
      "epoch: 1300, Accuracy: 0.750000\n",
      "epoch: 1400, Accuracy: 0.750000\n",
      "epoch: 1500, Accuracy: 0.500000\n",
      "epoch: 1600, Accuracy: 0.500000\n",
      "epoch: 1700, Accuracy: 0.500000\n",
      "epoch: 1800, Accuracy: 0.500000\n",
      "epoch: 1900, Accuracy: 0.500000\n",
      "epoch: 2000, Accuracy: 0.500000\n",
      "epoch: 2100, Accuracy: 0.500000\n",
      "epoch: 2200, Accuracy: 0.500000\n",
      "epoch: 2300, Accuracy: 0.500000\n",
      "epoch: 2400, Accuracy: 0.500000\n",
      "epoch: 2500, Accuracy: 0.500000\n",
      "epoch: 2600, Accuracy: 0.500000\n",
      "epoch: 2700, Accuracy: 0.500000\n",
      "epoch: 2800, Accuracy: 0.500000\n",
      "epoch: 2900, Accuracy: 0.500000\n",
      "epoch: 3000, Accuracy: 0.500000\n",
      "epoch: 3100, Accuracy: 0.500000\n",
      "epoch: 3200, Accuracy: 0.500000\n",
      "epoch: 3300, Accuracy: 0.500000\n",
      "epoch: 3400, Accuracy: 0.500000\n",
      "epoch: 3500, Accuracy: 0.500000\n",
      "epoch: 3600, Accuracy: 0.500000\n",
      "epoch: 3700, Accuracy: 0.500000\n",
      "epoch: 3800, Accuracy: 0.500000\n",
      "epoch: 3900, Accuracy: 0.500000\n",
      "epoch: 4000, Accuracy: 0.500000\n",
      "epoch: 4100, Accuracy: 0.500000\n",
      "epoch: 4200, Accuracy: 0.500000\n",
      "epoch: 4300, Accuracy: 0.500000\n",
      "epoch: 4400, Accuracy: 0.500000\n",
      "epoch: 4500, Accuracy: 0.500000\n",
      "epoch: 4600, Accuracy: 0.500000\n",
      "epoch: 4700, Accuracy: 0.500000\n",
      "epoch: 4800, Accuracy: 0.500000\n",
      "epoch: 4900, Accuracy: 0.500000\n",
      "epoch: 5000, Accuracy: 0.500000\n",
      "epoch: 5100, Accuracy: 0.500000\n",
      "epoch: 5200, Accuracy: 0.500000\n",
      "epoch: 5300, Accuracy: 0.500000\n",
      "epoch: 5400, Accuracy: 0.500000\n",
      "epoch: 5500, Accuracy: 0.500000\n",
      "epoch: 5600, Accuracy: 0.500000\n",
      "epoch: 5700, Accuracy: 0.500000\n",
      "epoch: 5800, Accuracy: 0.500000\n",
      "epoch: 5900, Accuracy: 0.500000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(6000):\n",
    "    sess.run(train_step,feed_dict={\n",
    "        x:x_train,\n",
    "        t:y_train\n",
    "    })\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        acc_val = sess.run(\n",
    "            accuracy,feed_dict={\n",
    "                x:x_train,\n",
    "                t:y_train})\n",
    "        print ('epoch: %d, Accuracy: %f'\n",
    "               %(epoch, acc_val))\n",
    "\n",
    "classified = correct_prediction.eval(session=sess, feed_dict={\n",
    "    x:x_train,\n",
    "    t:y_train\n",
    "})\n",
    "\n",
    "prob = y.eval(session=sess,feed_dict={\n",
    "    x:x_train,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [False]\n",
      " [ True]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "print(classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00266801]\n",
      " [0.49924862]\n",
      " [0.99617094]\n",
      " [0.5019172 ]]\n"
     ]
    }
   ],
   "source": [
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: [[-5.559029   3.3968854]\n",
      " [-8.926386  -8.54109  ]]\n",
      "b1: [ 1.5105188 -1.9390633]\n",
      "W2: [[-8.312973]\n",
      " [ 7.029182]]\n",
      "b2: [0.00179539]\n"
     ]
    }
   ],
   "source": [
    "print('W1:', sess.run(W1))\n",
    "print('b1:', sess.run(b1))\n",
    "print('W2:', sess.run(W2))\n",
    "print('b2:', sess.run(b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientDescentとは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの学習では、誤差関数が最小になるように最適なパラメータ(重み、バイアス)を探索します。このパラメータの探索方法の一つにGradientDescent(勾配降下法)と呼ばれる手法があります。これは名前の通り誤差関数のGradient(勾配)を利用して最適化パラメータを見つけ出す手法です。勾配は誤差関数を微分することによって求めることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一変数の考え方は簡単です。\n",
    "下に凸の二次関数の場合、与えられた初期値での微分が正の場合左へ、負の場合右へ進みます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "勾配降下法は方程式から解を直接求めるのではなく、イメージ的には上図のように、少しずつ点を動かしながら最小値を探し出していく方法です。\n",
    "\n",
    "※GradientDescentは関数の最小値を見つける手法です。上に凸の関数の最大値を探す手法はGradientAscentと呼ばれます。ニューラルネットワークで使われる誤差関数は下に凸の関数であるため、GradientDescentを使います。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientDescentの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'GradientDescent_6' type=NoOp>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章までではあまり触れなかったのでここでさらに詳しく見て行きましょう。\n",
    "まずはtf.train.GradientDescentOptimizerの次に出てくる(0.1)についてです。これが上式で出てきた学習率を指定する部分になります。学習率はハイパーパラメータなのでした。よってこの学習率をどのくらいにするか自分で決める必要があります。一般的に0.1,0.01,0.001など小さい値を用います。この値が小さすぎても大きすぎても最適なパラメータをうまく見つけられなさそうなことは直感的にわかると思います。ハイパーパラメータに関しては、自分で値を変更していき、学習が正しく、精度よく行われているか確認しながら学習を進めていく必要があります。\n",
    "そして.minimize()で最小化させる関数を指定しています。\n",
    "このコードは、GradientDescentを使って誤差関数(cross_entropy)を最小化させるということを行なっていることがなんとなくわかると思います。\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagationとは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backpropagation(誤差逆伝播法)とは、多数のパラメータを持つ関数の各パラメータの微分係数を高速に計算するためのアルゴリズムです。\n",
    "機械学習で用いるGradientDiscentでは，パラメータの更新に誤差関数の勾配(偏微分係数)を計算する必要がありました。通常、コンピュータで係数を計算する場合、数値微分というアルゴリズムで計算を行います。\n",
    "ただ、NeuralNetworkのパラメータは非常に多くなります。例えば、特徴量が100個あった場合、\n",
    "\n",
    "$y = w_0 + w_1 x_1 + \\dots + w_{100} x_{100}$\n",
    "\n",
    "において、誤差関数の偏微分係数を計算していくという作業になるので、大変計算時間がかかります。\n",
    "そこで考案された高速な手法がbackpropagationという手法になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.summary.writer.writer.FileWriter at 0xb3322e860>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.FileWriter('XOR', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行が終わったら、ターミナルを開いて以下を打ち込みましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ tensorboard --logdir=/ディレクトリの絶対パス/XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ディレクトリのパスはFinderからターミナルにXORディレクトリをドラッグアンドドロップすれば取得できます。\n",
    "起動できたら、localhost:6006にアクセスすれば、TensorBoardが表示されます。\n",
    "    \n",
    "Chromeで起動するのが望ましいです。Safariだとうまく起動しないことがあります。(ブラウザにlocalhost:6006と入れれば出てきます。)\n",
    "可視化することができました。出てきたグラフを見てみましょう。\n",
    "\n",
    "グラフは表示できていますが、何が何を表しているのか全然わからないと思います。\n",
    "\n",
    "http://hirokisuzaki.local:6006/#graphs&run=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None,2], name='x')\n",
    "t = tf.placeholder(tf.float32, [None,1], name='t')\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([2,2]), name='W1')\n",
    "b1 = tf.Variable(tf.zeros([2]), name='b1')\n",
    "h = tf.nn.sigmoid(tf.matmul(x,W1) + b1, name='h')\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([2,1]), name='W2')\n",
    "b2 = tf.Variable(tf.zeros([1]), name='b2')\n",
    "y = tf.nn.sigmoid(tf.matmul(h, W2) + b2, name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('cross_entropy'):\n",
    "  cross_entropy = -tf.reduce_sum(t * tf.log(y) + (1 - t) * tf.log(1 - y))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "  train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "  correct_prediction = tf.equal(tf.sign(y-0.5),tf.sign(t-0.5))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.name_scope()は、誤差の計算や正解率の計算など、計算処理をひとまとめにして名前をつけて表示させたい時に使います。\n",
    "\n",
    "※\n",
    "https://www.tensorflow.org/api_docs/python/tf/name_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "としましょう。この結果はTensorBoardのSCALARSというところに記録されます。sess.run()の中にmergedを入れて実行します。今回はエポックごとにcross_entropyとaccuracyの値も得たいのでこの2つも入れます。.最後.add_summary()によって、ここで取得したものをディレクトリに書き込んでいきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'biases_out_2:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.histogram('weights_h', W1)\n",
    "tf.summary.histogram('biases_h', b1)\n",
    "tf.summary.histogram('weights_out', W2)\n",
    "tf.summary.histogram('biases_out', b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.summary.writer.writer.FileWriter at 0xb33435c18>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.FileWriter('XOR', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
